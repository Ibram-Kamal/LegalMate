
# streamlit_rag_legalmate.py
import os
import streamlit as st
import pdfplumber
from langchain.text_splitter import RecursiveCharacterTextSplitter
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
from google import genai

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
# Configuration
# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
API_KEY = os.getenv("GENAI_API_KEY", "PUT_HERE_YOUR_API")
MODEL_ID = "gemini-2.5-flash-preview-04-17"

if not API_KEY or API_KEY.startswith("YOUR_"):
    st.error("Please set your GENAI_API_KEY environment variable.")
    st.stop()

client = genai.Client(api_key=API_KEY)

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
# Helper: extract text from uploaded file
# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
def extract_text(uploaded_file):
    if uploaded_file.type == "text/plain":
        return uploaded_file.read().decode("utf-8")
    elif uploaded_file.type == "application/pdf":
        text = ""
        with pdfplumber.open(uploaded_file) as pdf:
            for page in pdf.pages:
                page_text = page.extract_text()
                if page_text:
                    text += page_text + "\n"
        return text
    return ""

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
# Streamlit App
# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
st.set_page_config(page_title="Legal Mate", layout="wide")
st.title("ğŸ“œ Legal Mate")

# Initialize chat history
if "history" not in st.session_state:
    st.session_state.history = []

# 1. File Uploader
uploaded_file = st.file_uploader("Upload Arabic contract (PDF or TXT)", type=["pdf","txt"], key="file_uploader")

# If a new file is uploaded (different from session), reset everything
if uploaded_file and uploaded_file != st.session_state.get("current_file"):
    # reset states for new contract
    st.session_state.current_file = uploaded_file
    st.session_state.contract_text = extract_text(uploaded_file)
    st.session_state.chunks = None
    st.session_state.embeddings = None
    st.session_state.index = None
    st.session_state.chunk_map = {}
    st.session_state.embed_model = None
    # clear history of Q&A as needed or keep
    # st.session_state.history = []  # uncomment to reset chat history on new file

if not uploaded_file:
    st.info("Please upload a contract file to begin.")
    st.stop()

# 2. Initialize Session State for contract processing (after file check)
if "contract_text" not in st.session_state:
    # in case session restored without new upload
    st.session_state.contract_text = extract_text(uploaded_file)
    st.session_state.chunks = None
    st.session_state.embeddings = None
    st.session_state.index = None
    st.session_state.chunk_map = {}
    st.session_state.embed_model = None
if "contract_text" not in st.session_state:
    st.session_state.contract_text = extract_text(uploaded_file)
    st.session_state.chunks = None
    st.session_state.embeddings = None
    st.session_state.index = None
    st.session_state.chunk_map = {}
    st.session_state.embed_model = None

# 3. Split into Chunks
if st.session_state.chunks is None:
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=50
    )
    st.session_state.chunks = splitter.split_text(st.session_state.contract_text)
    st.success(f"Contract split into {len(st.session_state.chunks)} chunks.")

# 4. Initialize Chat & System Prompt (once per file)
if "chat" not in st.session_state:
    # Create new chat session and send system prompt
    st.session_state.chat = client.chats.create(model=MODEL_ID)
    # build & save system prompt
    system_prompt = f"""
Ø£Ù†ØªÙ â€œÙ„ÙŠØ¬Ø§Ù„ Ù…ÙŠØªâ€ØŒ Ù…Ø³Ø§Ø¹Ø¯ÙŒ Ø°ÙƒÙÙŠÙ‘ÙŒ Ù…Ø®ØªØµÙ‘ÙŒ ÙÙŠ Ø§Ù„Ø¹Ù‚ÙˆØ¯ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠÙ‘Ø© Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠÙ‘Ø©.

1. Ù‚Ø³Ù‘Ù… Ø§Ù„Ù†ØµÙ‘ Ø§Ù„Ø¹Ù‚Ø¯ÙŠÙ‘ Ø¥Ù„Ù‰ Ø¹Ù†Ø§ÙˆÙŠÙ† ÙˆØ¨Ù†ÙˆØ¯ Ù…Ø±Ù‚Ù‘Ù…Ø©.
2. Ø¨Ø§Ù„Ù†Ø³Ø¨Ø© Ù„ÙƒÙ„Ù‘ Ø¨Ù†Ø¯:
   a) Ø£Ù„Ø®ÙØµÙ‡ Ø¨Ù„ØºØ©Ù Ø¹Ø±Ø¨ÙŠÙ‘Ø©Ù ÙØµØ­Ù‰Ù ÙˆØ§Ø¶Ø­Ø© ÙˆØ¨Ø³ÙŠØ·Ø© ÙˆØ¯Ù‚ÙŠÙ‚Ø©.
   b) Ø­Ø¯Ù‘Ø¯ Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ø¶Ø¹ÙŠÙØ© Ø£Ùˆ Ø§Ù„Ù…Ø¨Ù‡Ù…Ø©.
   c) ØµÙ†Ù‘Ù Ø®Ø·ÙˆØ±Ø© ÙƒÙ„Ù‘ Ù†Ù‚Ø·Ø©: (Ù…Ù†Ø®ÙØ¶Ø© / Ù…ØªÙˆØ³Ù‘Ø·Ø© / Ø¹Ø§Ù„ÙŠØ©).
   d) Ù‚Ø¯Ù‘Ù… ØªÙˆØµÙŠØ©Ù‹ Ø£Ùˆ ØªÙˆØµÙŠØªÙŠÙ† Ø¹Ù…Ù„ÙŠÙ‘ØªÙŠÙ†.

Ø§Ù„ØªÙ†Ø³ÙŠÙ‚ Ø¨Ø¯Ù‚Ø© ÙƒÙ…Ø§ ÙŠÙ„ÙŠ:

Ø§Ù„Ø¨Ù†Ø¯ 1:
â€¢ Ø§Ù„Ù…Ù„Ø®Øµ: â€¦
â€¢ Ù†Ù‚Ø§Ø· Ø§Ù„Ø¶Ø¹Ù ÙˆØ§Ù„Ù…Ø®Ø§Ø·Ø±:

1. â€¦ â€” Ø§Ù„Ø®Ø·ÙˆØ±Ø©: (Ù…Ù†Ø®ÙØ¶Ø©/Ù…ØªÙˆØ³Ù‘Ø·Ø©/Ø¹Ø§Ù„ÙŠØ©)
2. â€¦ â€” Ø§Ù„Ø®Ø·ÙˆØ±Ø©: (Ù…Ù†Ø®ÙØ¶Ø©/Ù…ØªÙˆØ³Ù‘Ø·Ø©/Ø¹Ø§Ù„ÙŠØ©)
â€¢ Ø§Ù„ØªÙˆØµÙŠØ§Øª:
3. â€¦
4. â€¦

## Ù†Øµ Ø§Ù„Ø¹Ù‚Ø¯:
---
{st.session_state.contract_text}
---
"""
    st.session_state.chat.send_message(system_prompt)
    st.session_state.system_prompt = system_prompt
    system_prompt = f"""
Ø£Ù†ØªÙ â€œÙ„ÙŠØ¬Ø§Ù„ Ù…ÙŠØªâ€ØŒ Ù…Ø³Ø§Ø¹Ø¯ÙŒ Ø°ÙƒÙÙŠÙ‘ÙŒ Ù…Ø®ØªØµÙ‘ÙŒ ÙÙŠ Ø§Ù„Ø¹Ù‚ÙˆØ¯ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠÙ‘Ø© Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠÙ‘Ø©.

1. Ù‚Ø³Ù‘Ù… Ø§Ù„Ù†ØµÙ‘ Ø§Ù„Ø¹Ù‚Ø¯ÙŠÙ‘ Ø¥Ù„Ù‰ Ø¹Ù†Ø§ÙˆÙŠÙ† ÙˆØ¨Ù†ÙˆØ¯ Ù…Ø±Ù‚Ù‘Ù…Ø©.
2. Ø¨Ø§Ù„Ù†Ø³Ø¨Ø© Ù„ÙƒÙ„Ù‘ Ø¨Ù†Ø¯:
   a) Ø£Ù„Ø®ÙØµÙ‡ Ø¨Ù„ØºØ©Ù Ø¹Ø±Ø¨ÙŠÙ‘Ø©Ù ÙØµØ­Ù‰Ù ÙˆØ§Ø¶Ø­Ø© ÙˆØ¨Ø³ÙŠØ·Ø© ÙˆØ¯Ù‚ÙŠÙ‚Ø©.
   b) Ø­Ø¯Ù‘Ø¯ Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ø¶Ø¹ÙŠÙØ© Ø£Ùˆ Ø§Ù„Ù…Ø¨Ù‡Ù…Ø©.
   c) ØµÙ†Ù‘Ù Ø®Ø·ÙˆØ±Ø© ÙƒÙ„Ù‘ Ù†Ù‚Ø·Ø©: (Ù…Ù†Ø®ÙØ¶Ø© / Ù…ØªÙˆØ³Ù‘Ø·Ø© / Ø¹Ø§Ù„ÙŠØ©).
   d) Ù‚Ø¯Ù‘Ù… ØªÙˆØµÙŠØ©Ù‹ Ø£Ùˆ ØªÙˆØµÙŠØªÙŠÙ† Ø¹Ù…Ù„ÙŠÙ‘ØªÙŠÙ†.

Ø§Ù„ØªÙ†Ø³ÙŠÙ‚ Ø¨Ø¯Ù‚Ø© ÙƒÙ…Ø§ ÙŠÙ„ÙŠ:

Ø§Ù„Ø¨Ù†Ø¯ 1:
â€¢ Ø§Ù„Ù…Ù„Ø®Øµ: â€¦
â€¢ Ù†Ù‚Ø§Ø· Ø§Ù„Ø¶Ø¹Ù ÙˆØ§Ù„Ù…Ø®Ø§Ø·Ø±:

1. â€¦ â€” Ø§Ù„Ø®Ø·ÙˆØ±Ø©: (Ù…Ù†Ø®ÙØ¶Ø©/Ù…ØªÙˆØ³Ù‘Ø·Ø©/Ø¹Ø§Ù„ÙŠØ©)
2. â€¦ â€” Ø§Ù„Ø®Ø·ÙˆØ±Ø©: (Ù…Ù†Ø®ÙØ¶Ø©/Ù…ØªÙˆØ³Ù‘Ø·Ø©/Ø¹Ø§Ù„ÙŠØ©)
â€¢ Ø§Ù„ØªÙˆØµÙŠØ§Øª:
3. â€¦
4. â€¦

## Ù†Øµ Ø§Ù„Ø¹Ù‚Ø¯:
---
{st.session_state.contract_text}
---
"""
    st.session_state.chat.send_message(system_prompt)  # send system prompt (no author arg)

# 5. Compute Embeddings & Build FAISS Index Compute Embeddings & Build FAISS Index
if st.session_state.index is None:
    st.session_state.embed_model = SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2")
    st.session_state.embeddings = st.session_state.embed_model.encode(
        st.session_state.chunks, show_progress_bar=True
    )
    dim = st.session_state.embeddings.shape[1]
    index = faiss.IndexFlatL2(dim)
    index.add(np.array(st.session_state.embeddings))
    st.session_state.index = index
    st.session_state.chunk_map = {i: c for i, c in enumerate(st.session_state.chunks)}
    st.success("Embeddings generated and FAISS index built.")

# Display existing chat history
for entry in st.session_state.history:
    st.chat_message(entry["role"]).write(entry["content"])

# 5. Action Buttons & Chat Input
# Define buttons and record selection in session_state
col1, col2, col3, col4 = st.columns(4)
if col1.button("ğŸ” ØªØ­Ù„ÙŠÙ„ Ù…Ø¨Ø³Ø· Ù„Ù„Ø¹Ù‚Ø¯", key="btn_simple"):
    st.session_state.selected_action = "simple"
if col2.button("ğŸ‘¥ Ø£Ø·Ø±Ø§Ù Ø§Ù„Ø¹Ù‚Ø¯", key="btn_parties"):
    st.session_state.selected_action = "parties"
if col3.button("ğŸ“„ Ù†ÙˆØ¹ Ø§Ù„Ø¹Ù‚Ø¯", key="btn_type"):
    st.session_state.selected_action = "type"
if col4.button("ğŸ§ ØªØ­Ù„ÙŠÙ„ ØªÙØµÙŠÙ„ÙŠ Ù„Ù„Ø¹Ù‚Ø¯", key="btn_detailed"):
    st.session_state.selected_action = "detailed"

# Determine the prompt based on selected action
generate_prompt = None
action = st.session_state.get("selected_action")
if action == "simple":
    generate_prompt = "Ø­Ù„Ù„ Ù„ÙŠ Ù‡Ø°Ø§ Ø§Ù„Ø¹Ù‚Ø¯ Ø¨Ù„ØºØ© Ø¹Ø±Ø¨ÙŠØ© ÙØµØ­Ù‰ Ø¨Ø³ÙŠØ·Ø© ÙˆØ³Ù‡Ù„Ø© Ø§Ù„ÙÙ‡Ù…."
elif action == "parties":
    generate_prompt = "Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ø£Ø·Ø±Ø§Ù Ø§Ù„Ù…Ø´Ø§Ø±ÙƒØ© ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ø¹Ù‚Ø¯ ÙˆÙ…Ø§ Ø£Ø¯ÙˆØ§Ø±Ù‡Ù…ØŸ"
elif action == "type":
    generate_prompt = "Ù…Ø§ Ù†ÙˆØ¹ Ù‡Ø°Ø§ Ø§Ù„Ø¹Ù‚Ø¯ (Ù…Ø«Ù„Ø§Ù‹: Ø¨ÙŠØ¹ØŒ Ø¥ÙŠØ¬Ø§Ø±ØŒ Ø®Ø¯Ù…Ø§Øª) ÙˆÙ„Ù…Ø§Ø°Ø§ØŸ"
elif action == "detailed":
    # use the full system prompt for detailed analysis
    generate_prompt = "Ø§Ø±ÙŠØ¯ ØªØ­Ù„ÙŠÙ„ ØªÙØµÙŠÙ„ÙŠ Ù„Ù‡Ø°Ø§ Ø§Ù„Ø¹Ù‚Ø¯"
    generate_prompt = st.session_state.system_prompt

# Always provide a chat input for custom questions
custom_q = st.chat_input("ğŸ“© ÙŠÙ…ÙƒÙ†Ùƒ ÙƒØªØ§Ø¨Ø© Ø³Ø¤Ø§Ù„Ùƒ Ø§Ù„Ø®Ø§Øµ Ù‡Ù†Ø§â€¦", key="chat_input")

# Final user_question: prefer button action, else custom input
user_question = generate_prompt if generate_prompt else custom_q

if user_question:
    # Do not echo the detailed analysis system_prompt in chat
    if action != "detailed":
        # Record and display user message
        st.session_state.history.append({"role": "user", "content": user_question})
        st.chat_message("user").write(user_question)
    else:
        # For detailed analysis, only record to history without GUI echo
        st.session_state.history.append({"role": "user", "content": "<ØªØ­Ù„ÙŠÙ„ ØªÙØµÙŠÙ„ÙŠ Ù„Ù„Ø¹Ù‚Ø¯>"})

    # 6. Retrieve all chunks in order Retrieve all chunks in order Retrieve all chunks in order
    total_chunks = len(st.session_state.chunks)
    q_vec = st.session_state.embed_model.encode([user_question])
    distances, indices = st.session_state.index.search(
        np.array(q_vec), k=total_chunks
    )
    all_indices = sorted(indices[0])
    retrieved = [st.session_state.chunk_map[idx] for idx in all_indices]
    context = "".join(retrieved)

    # 7. Build System Prompt + Context
    system_prompter = (
        "Ø£Ù†ØªÙ Ù„ÙŠØ¬Ø§Ù„ Ù…ÙŠØª Ù…Ø³Ø§Ø¹Ø¯ Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø°ÙƒÙŠ Ù…Ø®ØªØµÙ‘ Ø¨Ø§Ù„Ø¹Ù‚ÙˆØ¯ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©. "
        "Ø§Ø³ØªØ¹Ù† Ø¨Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„ØªØ§Ù„ÙŠ Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ø¯Ù‚Ø© ÙˆØ¥ÙŠØ¬Ø§Ø²."
    )
    prompt = f"""
{system_prompter}

## Ù…Ù‚ØªØ·ÙØ§Øª Ù…Ù† Ø§Ù„Ø¹Ù‚Ø¯:
{context}

## Ø§Ù„Ø³Ø¤Ø§Ù„:
{user_question}
"""

    # 8. Send to Gemini with streaming and display
    # Send the user question only; system prompt is preloaded
    assistant_chat = st.session_state.chat
    reply_content = ""
    with st.chat_message("assistant"):
      placeholder = st.empty()
       
      for chunk in assistant_chat.send_message_stream(prompt):
    # Guard against None
        text_piece = chunk.text or ""
        reply_content += text_piece
        placeholder.write(reply_content)

    
    # Save assistant reply to history
    st.session_state.history.append({"role": "assistant", "content": reply_content})
    st.session_state.selected_action = None  # ğŸ” Reset action
